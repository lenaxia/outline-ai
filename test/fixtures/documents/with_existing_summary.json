{
  "id": "doc-with-summary-001",
  "collectionId": "col-engineering-001",
  "title": "Database Migration Strategy",
  "text": "# Database Migration Strategy\n\n<!-- AI-SUMMARY-START -->\n> **Summary**: This document outlines our approach to database migrations using zero-downtime techniques, automated rollback procedures, and comprehensive testing strategies to ensure safe schema changes in production.\n<!-- AI-SUMMARY-END -->\n\n## Overview\n\nDatabase migrations are a critical part of our deployment process. This document establishes our standards for planning, executing, and validating schema changes in production environments.\n\n## Guiding Principles\n\n1. **Zero Downtime**: All migrations must support continuous operation\n2. **Reversible**: Every change must have a rollback path\n3. **Tested**: Migrations tested in staging before production\n4. **Incremental**: Break large changes into small, safe steps\n5. **Monitored**: Track migration progress and performance impact\n\n## Migration Types\n\n### Additive Changes (Safe)\n\nThese can be deployed without downtime:\n\n- Adding new tables\n- Adding nullable columns\n- Adding indexes (use `CONCURRENTLY` in PostgreSQL)\n- Creating new foreign keys (if not enforced immediately)\n\n**Example - Adding Column**:\n```sql\n-- Step 1: Add nullable column\nALTER TABLE users ADD COLUMN phone_number VARCHAR(20);\n\n-- Step 2: Backfill data (in batches)\nUPDATE users \nSET phone_number = legacy_phone \nWHERE phone_number IS NULL \nLIMIT 1000;\n\n-- Step 3: Add NOT NULL constraint (after backfill complete)\nALTER TABLE users ALTER COLUMN phone_number SET NOT NULL;\n```\n\n### Destructive Changes (Risky)\n\nRequire careful planning:\n\n- Dropping columns\n- Dropping tables\n- Renaming columns\n- Changing column types\n- Adding NOT NULL constraints\n\n**Multi-Phase Approach**:\n\n**Phase 1: Deprecation**\n- Deploy code that stops writing to old column\n- Monitor for any remaining writes\n- Wait 1 week minimum\n\n**Phase 2: Removal**\n- Drop deprecated column/table\n- Monitor for errors\n- Keep backups for 30 days\n\n### Column Renames (Complex)\n\nRenaming requires multiple deployments:\n\n**Step 1: Add new column**\n```sql\nALTER TABLE users ADD COLUMN email_address VARCHAR(255);\n```\n\n**Step 2: Dual-write phase**\n```python\n# Application writes to both columns\ndef update_user(user_id, email):\n    db.execute(\n        \"UPDATE users SET email = %s, email_address = %s WHERE id = %s\",\n        (email, email, user_id)\n    )\n```\n\n**Step 3: Backfill**\n```sql\nUPDATE users SET email_address = email WHERE email_address IS NULL;\n```\n\n**Step 4: Switch to read from new column**\n```python\n# Application reads from new column\ndef get_user_email(user_id):\n    return db.query(\"SELECT email_address FROM users WHERE id = %s\", (user_id,))\n```\n\n**Step 5: Remove old column**\n```sql\nALTER TABLE users DROP COLUMN email;\n```\n\n## Migration Tools\n\n### Alembic (Python)\n\n```python\n# migrations/versions/001_add_phone_to_users.py\nfrom alembic import op\nimport sqlalchemy as sa\n\ndef upgrade():\n    op.add_column('users',\n        sa.Column('phone_number', sa.String(20), nullable=True)\n    )\n\ndef downgrade():\n    op.drop_column('users', 'phone_number')\n```\n\n### Flyway (Java)\n\n```sql\n-- V001__Add_phone_to_users.sql\nALTER TABLE users ADD COLUMN phone_number VARCHAR(20);\n```\n\n### go-migrate (Go)\n\n```sql\n-- 001_add_phone_to_users.up.sql\nALTER TABLE users ADD COLUMN phone_number VARCHAR(20);\n\n-- 001_add_phone_to_users.down.sql\nALTER TABLE users DROP COLUMN phone_number;\n```\n\n## Index Management\n\n### Creating Indexes\n\n**PostgreSQL**:\n```sql\n-- Build index without locking table\nCREATE INDEX CONCURRENTLY idx_users_email ON users(email);\n```\n\n**MySQL**:\n```sql\n-- InnoDB allows concurrent reads during index creation\nCREATE INDEX idx_users_email ON users(email) ALGORITHM=INPLACE;\n```\n\n### Dropping Indexes\n\n```sql\n-- PostgreSQL\nDROP INDEX CONCURRENTLY idx_users_email;\n\n-- MySQL\nDROP INDEX idx_users_email ON users ALGORITHM=INPLACE;\n```\n\n### Index Monitoring\n\nCheck index usage before dropping:\n\n```sql\n-- PostgreSQL: Find unused indexes\nSELECT \n    schemaname,\n    tablename,\n    indexname,\n    idx_scan,\n    idx_tup_read,\n    idx_tup_fetch\nFROM pg_stat_user_indexes\nWHERE idx_scan = 0\nORDER BY pg_relation_size(indexrelid) DESC;\n```\n\n## Data Backfilling\n\n### Batch Processing\n\n```python\ndef backfill_phone_numbers():\n    batch_size = 1000\n    offset = 0\n\n    while True:\n        # Process batch\n        rows_updated = db.execute(\"\"\"\n            UPDATE users \n            SET phone_number = legacy_phone\n            WHERE phone_number IS NULL \n              AND id > %s\n            ORDER BY id\n            LIMIT %s\n        \"\"\", (offset, batch_size))\n\n        if rows_updated == 0:\n            break\n\n        offset += batch_size\n\n        # Rate limiting\n        time.sleep(0.1)\n\n        # Progress logging\n        logger.info(f\"Backfilled {offset} rows\")\n```\n\n### Background Jobs\n\n```python\n# Celery task for async backfill\n@celery.task\ndef backfill_user_batch(start_id, end_id):\n    db.execute(\"\"\"\n        UPDATE users \n        SET phone_number = legacy_phone\n        WHERE id BETWEEN %s AND %s\n          AND phone_number IS NULL\n    \"\"\", (start_id, end_id))\n```\n\n## Testing Strategy\n\n### Pre-Migration Checks\n\n```python\ndef test_migration_001():\n    # Test upgrade\n    alembic.upgrade('head')\n\n    # Verify schema\n    assert column_exists('users', 'phone_number')\n    assert column_nullable('users', 'phone_number')\n\n    # Test data operations\n    user_id = create_test_user(phone_number='555-1234')\n    user = get_user(user_id)\n    assert user.phone_number == '555-1234'\n\n    # Test rollback\n    alembic.downgrade('-1')\n    assert not column_exists('users', 'phone_number')\n```\n\n### Load Testing\n\n```bash\n# Simulate production load during migration\nk6 run --vus 100 --duration 5m load-test.js &\n\n# Run migration\npython manage.py migrate\n\n# Monitor performance\nwatch -n 1 'psql -c \"SELECT * FROM pg_stat_activity;\"'\n```\n\n### Staging Validation\n\nBefore production:\n\n1. Restore production snapshot to staging\n2. Run migration on staging\n3. Verify application functionality\n4. Check query performance\n5. Test rollback procedure\n\n## Rollback Procedures\n\n### Automatic Rollback\n\n```python\ntry:\n    with db.transaction():\n        # Run migration\n        alembic.upgrade('head')\n\n        # Verify critical functionality\n        if not verify_migration():\n            raise Exception('Verification failed')\nexcept Exception as e:\n    logger.error(f'Migration failed: {e}')\n    alembic.downgrade('-1')\n    raise\n```\n\n### Manual Rollback\n\n```bash\n# Rollback one version\nalembic downgrade -1\n\n# Rollback to specific version\nalembic downgrade abc123\n\n# Rollback all migrations\nalembic downgrade base\n```\n\n### Point-in-Time Recovery\n\n```bash\n# PostgreSQL PITR\npg_basebackup -D /backup/base -Ft -z -P\n\n# Restore to point before migration\npsql -c \"SELECT pg_create_restore_point('before_migration');\"\n```\n\n## Monitoring\n\n### Migration Progress\n\n```sql\n-- Track long-running queries\nSELECT \n    pid,\n    now() - query_start as duration,\n    query\nFROM pg_stat_activity\nWHERE state = 'active'\nORDER BY duration DESC;\n```\n\n### Performance Impact\n\n```python\nimport psutil\nimport time\n\ndef monitor_migration():\n    start_time = time.time()\n    start_cpu = psutil.cpu_percent()\n    start_memory = psutil.virtual_memory().percent\n\n    # Run migration\n    run_migration()\n\n    duration = time.time() - start_time\n    cpu_delta = psutil.cpu_percent() - start_cpu\n    memory_delta = psutil.virtual_memory().percent - start_memory\n\n    logger.info(f\"Migration completed in {duration:.2f}s\")\n    logger.info(f\"CPU impact: {cpu_delta:.1f}%\")\n    logger.info(f\"Memory impact: {memory_delta:.1f}%\")\n```\n\n### Alerting\n\nSet up alerts for:\n\n- Migration duration > expected time\n- Database connection pool exhaustion\n- Query timeout increases\n- Replication lag (for replicated databases)\n- Error rate spikes\n\n## Common Patterns\n\n### Expand-Contract Pattern\n\n1. **Expand**: Add new schema elements\n2. **Migrate**: Application uses both old and new\n3. **Contract**: Remove old schema elements\n\n### Blue-Green Database\n\n1. Create new database with migrated schema\n2. Replicate data to new database\n3. Switch application to new database\n4. Keep old database for rollback\n\n### Shadow Database\n\n1. Mirror writes to both old and new schema\n2. Compare results for validation\n3. Switch reads to new schema when confident\n4. Deprecate old schema\n\n## Checklist\n\nBefore every migration:\n\n- [ ] Migration tested in local environment\n- [ ] Migration tested in staging with production data snapshot\n- [ ] Rollback procedure documented and tested\n- [ ] Team notified of maintenance window\n- [ ] Monitoring dashboards prepared\n- [ ] On-call engineer assigned\n- [ ] Database backup verified\n- [ ] Load test completed\n- [ ] Performance baseline captured\n- [ ] Communication sent to stakeholders\n\n## Incident Response\n\nIf migration fails:\n\n1. **Stop**: Halt migration immediately\n2. **Assess**: Check database state and application health\n3. **Rollback**: Execute rollback procedure if needed\n4. **Communicate**: Update stakeholders on status\n5. **Debug**: Analyze logs and errors\n6. **Fix**: Correct issues in new migration version\n7. **Retry**: After thorough testing\n\n## Best Practices\n\n1. **Small Changes**: Break large migrations into small steps\n2. **Off-Peak Hours**: Run migrations during low traffic\n3. **Rate Limiting**: Throttle batch operations\n4. **Monitoring**: Watch metrics throughout migration\n5. **Documentation**: Keep detailed migration logs\n6. **Version Control**: All migrations in git\n7. **Idempotency**: Migrations can be run multiple times safely\n8. **Communication**: Notify team before and after\n\n## References\n\n- [PostgreSQL Zero Downtime Migrations](https://www.postgresql.org/docs/current/sql-altertable.html)\n- [MySQL Online DDL](https://dev.mysql.com/doc/refman/8.0/en/innodb-online-ddl.html)\n- [Database Reliability Engineering (Book)](https://www.oreilly.com/library/view/database-reliability-engineering/9781491925935/)\n\n---\n\n<!-- AI-SEARCH-TERMS-START -->\n**Search Terms**: database, migration, schema change, zero downtime, rollback, PostgreSQL, MySQL, Alembic, Flyway, DDL, backfill, index creation\n<!-- AI-SEARCH-TERMS-END -->\n\n---\n\n**Status**: Approved\n**Owner**: Platform Engineering Team\n**Last Updated**: January 18, 2024\n**Next Review**: July 2024",
  "createdAt": "2024-01-10T13:00:00Z",
  "updatedAt": "2024-01-18T10:15:00Z",
  "publishedAt": "2024-01-12T09:00:00Z"
}
